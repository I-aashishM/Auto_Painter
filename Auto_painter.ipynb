{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "ig_rz5BHR07e",
    "outputId": "9d164dd3-b0dd-4b55-f8ce-6019a9618d1f"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "LKOehL5bR0cj",
    "outputId": "126335dc-d1e3-46b1-a4dd-4bd5fdd840fc"
   },
   "outputs": [],
   "source": [
    "! kaggle datasets download -d ktaebum/anime-sketch-colorization-pair #download the dateset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Qh6acHh-Scmk",
    "outputId": "180cf517-1aa8-48ea-935a-242068507c1d"
   },
   "outputs": [],
   "source": [
    "! unzip anime-sketch-colorization-pair.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cljZdSgoVBNu"
   },
   "outputs": [],
   "source": [
    "! mkdir data_2\n",
    "\n",
    "! mkdir data_2/train\n",
    "! mkdir data_2/train/Images\n",
    "! mkdir data_2/train/Sketch\n",
    "\n",
    "!mkdir data_2/validation\n",
    "!mkdir data_2/validation/Images\n",
    "!mkdir data_2/validation/Sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8Ksd90fz1xY2",
    "outputId": "03c2c8dd-8b8c-4c0b-e0ea-4ccb09ab290e"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8tHs_bzhbAOT"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Importing the required libraries.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-klVOktiS-7T",
    "outputId": "19807cf3-535f-4356-9ba9-17c5ee8e51d0"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Displaying the sample sketch and color images.\n",
    "\"\"\"\n",
    "for file in glob.glob('data/train/*.png')[:5]:\n",
    "    f, a = plt.subplots(1,2, figsize=(10,5))\n",
    "    a = a.flatten()\n",
    "    \n",
    "    img = Image.open(file).convert('RGB')\n",
    "    a[0].imshow(img.crop((0, 0, 512,512))); a[0].axis('off');\n",
    "    a[1].imshow(img.crop((512, 0, 1024, 512))); a[1].axis('off');\n",
    "    \n",
    "    plt.show()\n",
    "    print(file)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Preprocessing and saving the training data to corresponding directory. \n",
    "\"\"\"\n",
    "for idx, file in tqdm(enumerate(glob.glob('data/train/*.png'))):\n",
    "    img = Image.open(file).convert('RGB')\n",
    "    \n",
    "    img.crop((0, 0, 512,512)).save('data_2/train/Images/{}.png'.format(idx))\n",
    "    img.crop((512, 0, 1024, 512)).save('data_2/train/Sketch/{}.png'.format(idx))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Preprocessing and saving the validation/test data to corresponding directory. \n",
    "\"\"\"\n",
    "for idx, file in tqdm(enumerate(glob.glob('data/val/*.png'))):\n",
    "    img = Image.open(file).convert('RGB')\n",
    "    \n",
    "    img.crop((0, 0, 512,512)).save('data_2/validation/Images/{}.png'.format(idx))\n",
    "    img.crop((512, 0, 1024, 512)).save('data_2/validation/Sketch/{}.png'.format(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zn73mCl8gt9J"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.activations import tanh, sigmoid\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import concatenate, ReLU, GlobalAveragePooling2D, Input, Dense, Dropout, Flatten, LeakyReLU, Conv2D, BatchNormalization, Conv2DTranspose\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKKobX1rixbm"
   },
   "outputs": [],
   "source": [
    "img_paths = glob.glob('data_2/train/Images/*.png')\n",
    "sketch_paths = glob.glob('data_2/train/Sketch/*.png')\n",
    "\n",
    "img_paths.sort()\n",
    "sketch_paths.sort()\n",
    "\n",
    "img_paths = np.array(img_paths)# [ids]\n",
    "sketch_paths = np.array(sketch_paths)# [ids]\n",
    "def generate_samples(sketch_paths, img_paths, n_samples):\n",
    "  \"\"\"\n",
    "  A function to load black-and-white sketches and colored images.\n",
    "  The function that loads the black-and-white sketches and corresponding colored images \n",
    "  from the given paths for training the GAN.\n",
    "  Parameters:\n",
    "    sketch_paths(numpy.array): The paths to the black-and-white sketches i.e input images.\n",
    "    image_paths(numpy.array): The paths to the colored images i.e target images.\n",
    "    n_samples(int): The # samples to load for training process.\n",
    "  Returns:\n",
    "    X_sketches(numpy.array): The loaded black-and-white sketches.\n",
    "    X_images(numpy.array): The loaded colored images.\n",
    "  \"\"\"\n",
    "\n",
    "  idxs = np.random.randint(0, 14223, n_samples)\n",
    "  # print(idxs)\n",
    "  X_sketches = []\n",
    "  X_images = []\n",
    "  \n",
    "  for sket, img in zip(sketch_paths[idxs], img_paths[idxs]):\n",
    "    X_sketches.append(np.array(Image.open(sket).convert('RGB')))\n",
    "    X_images.append(np.array(Image.open(img).convert('RGB')))\n",
    "  \n",
    "  # Normalizing the values to be between [-1, 1].\n",
    "  X_sketches = (np.array(X_sketches, dtype='float32')-127.5)/127.5\n",
    "  X_images = (np.array(X_images, dtype='float32')-127.5)/127.5\n",
    "\t\n",
    "  return  X_images, X_sketches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UQy4xphSLJH5"
   },
   "outputs": [],
   "source": [
    "if K.image_data_format == \"channels_first\":\n",
    "  input_dim = 3,512,512\n",
    "else:\n",
    "  input_dim = 512,512,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F5_yQ50oh1nu"
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr = 0.0002, beta_1 = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CRiEZrQO5NU_",
    "outputId": "0a8c2705-ae5f-4f9a-f89e-2d24876ca8b0"
   },
   "outputs": [],
   "source": [
    "input_1 = Input(input_dim)\n",
    "\n",
    "\n",
    "generator_1 = Conv2D(16, 4, strides=(2,2), padding='same',kernel_initializer=tf.keras.initializers.truncated_normal(stddev=.02))(input_1)\n",
    "activation_1 = LeakyReLU(0.2)(generator_1)\n",
    "\n",
    "generator_2 = Conv2D(32, 4, strides=(2,2), padding='same',kernel_initializer=tf.keras.initializers.truncated_normal(stddev=.02))(activation_1)\n",
    "batch_2 = BatchNormalization()(generator_2)\n",
    "activation_2 = LeakyReLU(0.2)(batch_2)\n",
    "\n",
    "generator_3 = Conv2D(64, 4, strides=(2,2), padding='same',kernel_initializer=tf.keras.initializers.truncated_normal(stddev=.02))(activation_2)\n",
    "batch_3 = BatchNormalization()(generator_3)\n",
    "activation_3 = LeakyReLU(0.2)(batch_3)\n",
    "\n",
    "\n",
    "generator_4 = Conv2D(128, 4, strides=(2,2), padding='same',kernel_initializer=tf.keras.initializers.truncated_normal(stddev=.02))(activation_3)\n",
    "batch_4 = BatchNormalization()(generator_4)\n",
    "activation_4 = LeakyReLU(0.2)(batch_4)\n",
    "\n",
    "generator_5 = Conv2D(128, 4, strides=(2,2), padding='same',kernel_initializer=tf.keras.initializers.truncated_normal(stddev=.02))(activation_4)\n",
    "batch_5 = BatchNormalization()(generator_5)\n",
    "activation_5 = LeakyReLU(0.2)(batch_5)\n",
    "\n",
    "generator_6 = Conv2D(128, 4, strides=(2,2), padding='same',kernel_initializer=tf.keras.initializers.truncated_normal(stddev=.02))(activation_5)\n",
    "batch_6 = BatchNormalization()(generator_6)\n",
    "activation_6 = LeakyReLU(0.2)(batch_6)\n",
    "\n",
    "generator_7 = Conv2D(128, 4, strides=(2,2), padding='same',kernel_initializer=tf.keras.initializers.truncated_normal(stddev=.02))(activation_6)\n",
    "batch_7 = BatchNormalization()(generator_7)\n",
    "activation_7 = LeakyReLU(0.2)(batch_7)\n",
    "\n",
    "generator_8 = Conv2D(128, 4, strides=(2,2), padding='same',kernel_initializer=tf.keras.initializers.truncated_normal(stddev=.02))(activation_7)\n",
    "batch_8 = BatchNormalization()(generator_8)\n",
    "activation_8 = LeakyReLU(0.2)(batch_8)\n",
    "\n",
    "##Decorder\n",
    "\n",
    "generator_9 = Conv2DTranspose(128, 4, strides=(2,2), padding='same',kernel_initializer=tf.keras.initializers.truncated_normal(stddev=.02))(activation_8)\n",
    "batch_9 = BatchNormalization()(generator_9)\n",
    "activation_9 = LeakyReLU(0.2)(batch_9)\n",
    "\n",
    "generator_10 = Conv2DTranspose(128, 4, strides=(2,2), padding='same',kernel_initializer=tf.keras.initializers.truncated_normal(stddev=.02))(concatenate([activation_9,generator_7]))\n",
    "batch_10 = BatchNormalization()(generator_10)\n",
    "activation_10 = LeakyReLU(0.2)(batch_10)\n",
    "dropout_1 = Dropout(0.5)(activation_10)\n",
    "\n",
    "\n",
    "generator_11 = Conv2DTranspose(128, 4, strides=(2,2), padding='same',kernel_initializer=tf.keras.initializers.truncated_normal(stddev=.02))(concatenate([dropout_1, generator_6]))\n",
    "batch_11 = BatchNormalization()(generator_11)\n",
    "activation_11 = LeakyReLU(0.2)(batch_11)\n",
    "dropout_2 = Dropout(0.5)(activation_11)\n",
    "\n",
    "\n",
    "\n",
    "generator_12 = Conv2DTranspose(128, 4, strides=(2,2), padding='same',kernel_initializer=tf.keras.initializers.truncated_normal(stddev=.02))(concatenate([dropout_2,generator_5]))\n",
    "batch_12 = BatchNormalization()(generator_12)\n",
    "activation_12 = LeakyReLU(0.2)(batch_12)\n",
    "dropout_3 = Dropout(0.5)(activation_12)\n",
    "\n",
    "\n",
    "generator_13 = Conv2DTranspose(64, 4, strides=(2,2), padding='same',kernel_initializer=tf.keras.initializers.truncated_normal(stddev=.02))(concatenate([dropout_3, generator_4]))\n",
    "batch_13 = BatchNormalization()(generator_13)\n",
    "activation_13 = LeakyReLU(0.2)(batch_13)\n",
    "\n",
    "generator_14 = Conv2DTranspose(32, 4, strides=(2,2), padding='same',kernel_initializer=tf.keras.initializers.truncated_normal(stddev=.02))(concatenate([activation_13, generator_3]))\n",
    "batch_14 = BatchNormalization()(generator_14)\n",
    "activation_14 = LeakyReLU(0.2)(batch_14)\n",
    "\n",
    "generator_15 = Conv2DTranspose(16, 4, strides=(2,2), padding='same',kernel_initializer=tf.keras.initializers.truncated_normal(stddev=.02))(concatenate([activation_14, generator_2]) )\n",
    "batch_15 = BatchNormalization()(generator_15)\n",
    "activation_15 = LeakyReLU(0.2)(batch_15)\n",
    "\n",
    "generator_16 = Conv2DTranspose(3, 4, padding='same', strides=(2,2),kernel_initializer=tf.keras.initializers.truncated_normal(stddev=.02))(concatenate([activation_15, generator_1]))\n",
    "activation_16 = tanh(generator_16)\n",
    "\n",
    "\n",
    "generator = Model(inputs = input_1, outputs = activation_16)\n",
    "\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "colab_type": "code",
    "id": "dWv3XBde-IEF",
    "outputId": "e2c86e43-4cce-4606-eedb-ee356588ae0a"
   },
   "outputs": [],
   "source": [
    "inp1 = Input(shape = input_dim) # sketch input\n",
    "inp2 = Input(shape = input_dim) # colored input\n",
    "\n",
    "inp = concatenate([inp1,inp2])\n",
    "\n",
    "discriminator_1 = Conv2D(16, 4, strides=(2,2),kernel_initializer=tf.keras.initializers.truncated_normal(stddev=.02), padding='same')(inp)\n",
    "dactivation_1 = LeakyReLU(0.2)(discriminator_1)\n",
    "\n",
    "discriminator_2 = Conv2D(32, 4, strides=(2,2),kernel_initializer=tf.keras.initializers.truncated_normal(stddev=.02), padding='same')(dactivation_1)\n",
    "dbatch_1 = BatchNormalization()(discriminator_2)\n",
    "dactivation_2 = LeakyReLU(0.2)(dbatch_1)\n",
    "\n",
    "discriminator_3 = Conv2D(64, 4, strides=(2,2),kernel_initializer=tf.keras.initializers.truncated_normal(stddev=.02), padding='same')(dactivation_2)\n",
    "dbatch_2 = BatchNormalization()(discriminator_3)\n",
    "dactivation_3 = LeakyReLU(0.2)(dbatch_2)\n",
    "\n",
    "discriminator_4 = Conv2D(128, 4, strides=(2,2),kernel_initializer=tf.keras.initializers.truncated_normal(stddev=.02), padding='same')(dactivation_3)\n",
    "dbatch_3 = BatchNormalization()(discriminator_4)\n",
    "dactivation_4 = LeakyReLU(0.2)(dbatch_3)\n",
    "\n",
    "discriminator_5 = Conv2D(128, 2, strides=(1,1),kernel_initializer=tf.keras.initializers.truncated_normal(stddev=.02), padding='valid')(dactivation_4)\n",
    "dbatch_4 = BatchNormalization()(discriminator_5)\n",
    "dactivation_5 = LeakyReLU(0.2)(dbatch_4)\n",
    "\n",
    "discriminator_6 = Conv2D(1, 2, strides=(1,1),kernel_initializer=tf.keras.initializers.truncated_normal(stddev=.02), padding='valid')(dactivation_5)\n",
    "dactivation_6 = sigmoid(discriminator_6)\n",
    "\n",
    "average_layer = GlobalAveragePooling2D()(dactivation_6)\n",
    "\n",
    "discriminator = Model(inputs = [inp1,inp2], outputs = average_layer)\n",
    "\n",
    "\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "2O3GsiR3xrz-",
    "outputId": "0866b0bb-17dc-4d98-ecc8-ecd66e1a7631"
   },
   "outputs": [],
   "source": [
    "vgg = VGG16(weights='imagenet')\n",
    "vgg_net1 = Model(inputs=vgg.input, outputs=ReLU()(vgg.get_layer('block2_conv2').output))\n",
    "vgg_net2 = Model(inputs=vgg.input, outputs=ReLU()(vgg.get_layer('block2_conv2').output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "q5ClHcKGuyct",
    "outputId": "9f82dd1f-1694-428e-f10b-b339f093d5d4"
   },
   "outputs": [],
   "source": [
    "vgg_net1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NPgyNdsDwmNE"
   },
   "outputs": [],
   "source": [
    "def totalVariation_loss(y, g):\n",
    "  \"\"\"\n",
    "  A loss for smoothness and to remove noise from the output image.\n",
    "  Custom loss for getting similar colors used in the training data. \n",
    "  Parameters:\n",
    "    y (Tensor): The target images to be generated.\n",
    "    g (Tensor): The output images by generator.\n",
    "  \n",
    "  Returns:\n",
    "    function: The reference to the loss function of prototype that \n",
    "      keras requires.\n",
    "  \"\"\"\n",
    "  import tensorflow.keras.backend as K\n",
    "  \n",
    "  def finalTVLoss(y_true, y_pred):\n",
    "    return K.abs( K.sqrt( K.sum(K.square(g[:, 1:, :, :] - g[:, :-1, :, :])) +\\\n",
    "                          K.sum(K.square(g[:, :, 1:, :] - g[:, :, :-1, :])) ) )\n",
    "  \n",
    "  return finalTVLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7M-nctYowqIm"
   },
   "outputs": [],
   "source": [
    "def featureLevel_loss(y, g):\n",
    "  \"\"\"\n",
    "  A loss for features extracted from 4th layer of VGG16.\n",
    "  Custom loss for extracting high level features of the target \n",
    "  colored and generated colored images.\n",
    "  Parameters:\n",
    "    y (Tensor): The target images to be generated.\n",
    "    g (Tensor): The output images by the generator.\n",
    "  \n",
    "  Returns:\n",
    "    function: The reference to the loss function of prototype \n",
    "      that keras requires.\n",
    "  \"\"\"\n",
    "  import tensorflow.keras.backend as K\n",
    "  \n",
    "  def finalFLoss(y_true, y_pred):\n",
    "    return K.mean( K.sqrt( K.sum( K.square( y - g ) ) ) )\n",
    "  \n",
    "  return finalFLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WSj68iNRwuRw"
   },
   "outputs": [],
   "source": [
    "def pixelLevel_loss(y, g):\n",
    "  \"\"\"\n",
    "  A loss for getting proper images by comparing each pixel.\n",
    "  Custom loss for Pixel2Pixel level translation so that colors don't \n",
    "  come out the edges of generated images.\n",
    "  Parameters:\n",
    "    y (Tensor): The real target images to be generated.\n",
    "    g (Tensor): The output images by the generator.\n",
    "  \n",
    "  Returns:\n",
    "    function: The reference to the loss function of the prototype \n",
    "      that keras requires.\n",
    "  \"\"\"\n",
    "  import tensorflow.keras.backend as K\n",
    "  \n",
    "  def finalPLLoss(y_true, y_pred):\n",
    "    return K.mean( K.abs( y - g ) )\n",
    "  \n",
    "  return finalPLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "g0_ZQWt7dxa4",
    "outputId": "64b10231-aa65-4354-a246-6c82f8754f2b"
   },
   "outputs": [],
   "source": [
    "discriminator.trainable = False\n",
    "\n",
    "ganInput = Input(input_dim)\n",
    "x = generator([ganInput])\n",
    "\n",
    "ganOutput = discriminator([ganInput,x])\n",
    "\n",
    "color_inp = Input(input_dim)\n",
    "\n",
    "pixelLevelLoss = pixelLevel_loss(color_inp, x) # pixel loss\n",
    "  \n",
    "\n",
    "totalVariationLoss = totalVariation_loss(color_inp, x)#total variation loss\n",
    " \n",
    "net1_outp = vgg_net1([tf.image.resize(color_inp, (224, 224), tf.image.ResizeMethod.BILINEAR)]) # feature loss\n",
    "net2_outp = vgg_net2([tf.image.resize(x, (224, 224), tf.image.ResizeMethod.BILINEAR)])\n",
    "\n",
    "featureLevelLoss = featureLevel_loss(net1_outp,net2_outp)\n",
    "\n",
    "\n",
    "\n",
    "gan = Model(inputs=[ganInput, color_inp], outputs=ganOutput)\n",
    "\n",
    "gan.compile(loss=lambda y_true, y_pred : tf.keras.losses.binary_crossentropy(y_true, y_pred) + \n",
    "                                             100 * pixelLevelLoss(y_true, y_pred) + \n",
    "                                             0.0001 * totalVariationLoss(y_true, y_pred) + \n",
    "                                             0.01 * featureLevelLoss(y_true, y_pred), optimizer=adam)\n",
    "\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Icklkpoz1YMH"
   },
   "outputs": [],
   "source": [
    "def generate_and_plot():\n",
    "  num_examples = 5\n",
    "  _,fake_image = generate_samples(sketch_paths, img_paths, 8)\n",
    "\n",
    "  generated_images = generator.predict(fake_image)\n",
    "  generated_images = (generated_images + 1) / 2.0\n",
    "\n",
    "\n",
    "  plt.figure(figsize=(10,10))\n",
    "  for i in range(num_examples):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(generated_images[i])\n",
    "    plt.axis(\"off\")\n",
    "  \n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xBJf8N2_j0Zz"
   },
   "outputs": [],
   "source": [
    "# d_total_loss = []\n",
    "def train(epochs = 1, batch_size = 8):\n",
    "  m = 14224 #number of training samples\n",
    "  batch_count = m//batch_size #1778\n",
    "\n",
    "  generate_and_plot()\n",
    "\n",
    "  for e in range(epochs):\n",
    "    print(f\"Epoch: {e}\")\n",
    "    for j in tqdm(range(batch_count)):\n",
    "      ##train Discriminator\n",
    "      if not j%2:\n",
    "        X_real_imgs, X_real_skets = generate_samples(sketch_paths, img_paths, 8)\n",
    "\n",
    "        y_real = np.zeros((batch_size,1))\n",
    "        y_real[:] = 0.9\n",
    "  \n",
    "        d_loss_1, _ = discriminator.train_on_batch([X_real_skets, X_real_imgs], y_real)\n",
    "      \n",
    "      if not j%3:\n",
    "\n",
    "        _, X_fake_skets = generate_samples(sketch_paths, img_paths, 8)\n",
    "\n",
    "        y_fake = np.zeros((batch_size,1))\n",
    "\n",
    "        generated_images = generator.predict(X_fake_skets) \n",
    "\n",
    "        d_loss_2, _ = discriminator.train_on_batch([X_fake_skets, generated_images], y_fake)\n",
    "\n",
    "    ##Gan training\n",
    "      X_gan_imgs, X_gan_skets  = generate_samples(sketch_paths, img_paths, 8)\n",
    "      \n",
    "      y_gan = np.ones((batch_size,1))\n",
    "      gan_loss = gan.train_on_batch([X_gan_skets, X_gan_imgs], y_gan)\n",
    "    \n",
    "    \n",
    "    if e%5==0:\n",
    "      print(d_loss_1,d_loss_2, gan_loss)\n",
    "      model_save_name = f'{e}_model.h5' ##saving model after 5 epochs\n",
    "      path = f\"/content/gdrive/My Drive/{model_save_name}\" \n",
    "      gan.save(path)\n",
    "      generate_and_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "b4Zrq9Eccaui",
    "outputId": "1ac53b5a-800f-4f25-b610-ad42a140cf38"
   },
   "outputs": [],
   "source": [
    "train(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UxdHuA5IL-ew"
   },
   "outputs": [],
   "source": [
    "img_paths = glob.glob('data_2/validation/Images/*.png')\n",
    "sketch_paths = glob.glob('data_2/validation/Sketch/*.png')\n",
    "\n",
    "img_paths.sort()\n",
    "sketch_paths.sort()\n",
    "\n",
    "img_paths = np.array(img_paths)# [ids]\n",
    "sketch_paths = np.array(sketch_paths)# [ids]\n",
    "\n",
    "def generate_validate(sketch_paths, img_paths, n_samples):\n",
    "  idxs = np.random.randint(0,3545 , n_samples)\n",
    "  # print(idxs)\n",
    "  X_sketches = []\n",
    "  X_images = []\n",
    "  \n",
    "  for sket, img in zip(sketch_paths[idxs], img_paths[idxs]):\n",
    "    X_sketches.append(np.array(Image.open(sket).convert('RGB')))\n",
    "    X_images.append(np.array(Image.open(img).convert('RGB')))\n",
    "  \n",
    "  # Normalizing the values to be between [-1, 1].\n",
    "  X_sketches = (np.array(X_sketches, dtype='float32')-127.5)/127.5\n",
    "  X_images = (np.array(X_images, dtype='float32')-127.5)/127.5\n",
    "\t\n",
    "  return  X_images, X_sketches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9131tc92yb2v"
   },
   "outputs": [],
   "source": [
    "def generate_and_plot_validate():\n",
    " \n",
    "  real_image,fake_image = generate_validate(sketch_paths, img_paths, 8)\n",
    "\n",
    "  generated_images = generator.predict(fake_image)\n",
    "  generated_images = (generated_images + 1) / 2.0\n",
    "  real_image = (real_image+1)/2.0\n",
    "  fake_image = (fake_image+1)/2.0\n",
    "\n",
    "  f, a = plt.subplots(8, 3, figsize=(12,60)); a = a.flatten()\n",
    "  idx = 0\n",
    " \n",
    "  for color,gen,sket in zip(real_image,generated_images,fake_image):\n",
    "    a[idx].imshow(color);a[idx].axis('off')\n",
    "    a[idx+1].imshow(gen); a[idx+1].axis('off')\n",
    "    a[idx+2].imshow(sket); a[idx+2].axis('off')\n",
    "    idx +=3\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CPDWAVzW1CNV",
    "outputId": "27d11af9-4f79-4807-9e3b-4e3f5d853833"
   },
   "outputs": [],
   "source": [
    "generate_and_plot_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XxMWMPCTGmSQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V3-TAG0h6Bhr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Untitled2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
